## Wikidata as a hub for the Linked Data cloud


This tutorial will teach beginners how to use Wikidata as a "linking hub" -- a starting point for exploring datasets by leveraging links both to Linked Data repositories and to datasets outside of the Linked Data cloud.  The tutorial will include lectures, demonstrations, and guided hands-on exercises.  

Recommended prerequisites
* An interest in working with data.
* Some understanding of the Linked Data model (RDF).
* Some knowledge of SPARQL query language (though we will use canned queries).

What to bring
* Your own laptop with a standard Web browser
* Your own Wikidata (or Wikipedia) account (see help on [creating an account](https://www.wikidata.org/w/index.php?title=Special:CreateAccount) and  [configuring the account for your language](https://www.wikidata.org/wiki/Help:Navigating_Wikidata/User_Options#Language_settings)). 

Schedule (four sessions of 90 minutes each):

1. **Introduction to Wikidata**.  Brief history of Wikidata, its community culture, data model, relation to Wikipedia, and relation to Linked Data. Demonstration of basic methods for discovering, browsing, querying, and editing Wikidata items.  Hands-on practice in creating new Wikidata items (for which real data will be provided).

2. **Wikidata as a hub for the Linked Data cloud**. Different types of links from Wikidata to resources in the Linked Data cloud, from trusted authorities to ordinary Web resources.  Uses of mapping properties ("exact match", "close match", "related match").  Finding, using, and proposing  Wikidata properties.  Demonstration of Mix'n'match, a tool for matching resources to be mapped.  Hands-on practice with Mix'n'match using prepared data.

3. **Applications based on Wikidata.**  How the coherence of data and precision of search can be improved by creating a semantic data models (entity schemas). Applications that use Wikidata as a back-end source of data and provide interfaces for formulating queries and contributing content. Participants will split into groups of two or three people each to work on exercises.

4. **Wikidata usage scenarios**.  Presentation of more advanced tools for creating links to datasets outside of Wikidata. Quality in Wikidata (e.g., duplicates) and quality control.  The role of references in Wikidata.  Use of application profiles, aka shape expressions, for ensuring the coherent description of items of a given type.  The use of bots for managing large-scale data initiatives such as Gene Wiki.  The evolving relationship of libraries to Wikidata.  First steps for starting a Wikidata projects.

#### Presenter bios
* **Andra Waagmeester** Is a biomedical computer scientist currently active in the Gene Wiki project, which aims at making Wikidata a central hub for knowledge in the linked data cloud for the life sciences. He is also part of the Shape Expressions (ShEx) community, which aims at creating and maintaining a formal language to schematically describe linked data.  
* **Joachim Neubert** works as a scientific software developer at ZBW - Leibniz Information Centre for Economics. In 2009, together with colleagues of hbz, he started the SWIB - Semantic Web in Libraries - conference. He is contributing to Wikidata, particularly in regard to interlinking authorities and knowledge organization systems, and has most recently started WikiProject 20th Century Press Archives.
* **Tom Baker** has worked for twenty years on metadata and Semantic Web standards such as Dublin Core, SKOS, AGROVOC, GACS, and Shape Expressions language (ShEx).  With a PhD in Anthropology from Stanford University, he has taught at AIT (Bangkok) and Sungkyunkwan University (Seoul) and worked at various research institutes in Italy and Germany.




